{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5dcdae-b4c5-49bf-908d-15b10f7351ae",
   "metadata": {},
   "source": [
    "# Projet TSUNAMI : Topological EEG-Based Functional Connectivity Analysis for Motor Imagery\n",
    "### Date : 2023-2024\n",
    "*Auteurs : Diane CHOUNLAMOUNTRY & Orane VERNET*\n",
    "\n",
    "*Encadrants : Mira RIZKALLAH, Aurélien VAN LANGHENHOVE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc76893-081a-4528-858e-c6d3efcac922",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "Dans ce notebook, on peut trouver toutes les fonctions que nous avons implémentées et les différentes méthodes que nous avons abordées afin de résoudre ce projet. Notons que nous n'avons pas forcément réussi à les rendre parfaitement discriminantes (distinction entre la main droite et la main gauche). Cependant, nous avons trouvé intéressant et utile de tout de même laisser ces méthodes, puisqu'elles pourront être reprises et améliorées par quiconque désirant poursuivre ce projet. Tous nos commentaires et remarques sont présents dans ce notebook afin de rendre nos résultats clairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae98d18-4c38-4d02-9c72-2b30494f0cda",
   "metadata": {},
   "source": [
    "# Etape 1 : importations des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2500f7-66b6-4794-9b34-cd4927536c4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install giotto-tda\n",
    "#!pip install mne\n",
    "#!pip install pandas\n",
    "#!pip install networkx\n",
    "#!pip install nxviz\n",
    "#!pip install community\n",
    "#!pip install gudhi\n",
    "#!pip install persim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35039a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries and packages\n",
    "import math\n",
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from nxviz import CircosPlot\n",
    "import community\n",
    "import gudhi\n",
    "from gudhi import representations\n",
    "import persim\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import sys\n",
    "from gtda.diagrams import PersistenceLandscape\n",
    "import gtda.homology as hl\n",
    "\n",
    "## Pour le machine learning\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91abbde-c567-410a-9c22-9801d8bde2d1",
   "metadata": {},
   "source": [
    "# Etape 2 : fonctions \n",
    "## Implémentation des différentes fonctions nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trials(data):\n",
    "    '''\n",
    "    Fonction permettant d'extraire des signaux EEG raw les epochs et les labels (1:droite ; 2: gauche) de chaque événement\n",
    "    '''\n",
    "    # Charger les annotations\n",
    "    annotations = data.annotations\n",
    "\n",
    "    # Extraire les événements à partir des annotations\n",
    "    events, event_id = mne.events_from_annotations(data, event_id={'769':1 , '770':2} ) #1=gauche et 2=droite\n",
    "\n",
    "    epochs = mne.Epochs(data, events, event_id, tmin=1, tmax=5,baseline=None) #l'essai entier dure 8sec, on ne s'intéresse qu'à ce moment (c'est modifiable)\n",
    "\n",
    "    label = events[:,2]\n",
    "    \n",
    "    return epochs.get_data(), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrice_de_correlation(essai):\n",
    "    '''\n",
    "    Prend en argument un essai et renvoie la matrice de correlation \n",
    "    de taille 32x32\n",
    "    '''\n",
    "    matrix = np.corrcoef(essai)\n",
    "\n",
    "    return(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78bd224-158b-4e5c-9436-a980b3e62012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_to_distance(matrice_corr, distance):\n",
    "    '''\n",
    "    Convertit une matrice de corrélation en une matrice de distance (distance choisie)\n",
    "    '''\n",
    "    if distance == '1':\n",
    "        return (1-np.absolute(matrice_corr))\n",
    "    if distance == '2':\n",
    "        return (np.sqrt(-np.log(matrice_corr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e090667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistence_barcode(matrice) : \n",
    "    '''\n",
    "    En entrée, prend en argument la matrice de distance\n",
    "    En sortie, plot le barcode correspondant\n",
    "    '''\n",
    "\n",
    "    rips_complex = gudhi.RipsComplex(distance_matrix=matrice, max_edge_length=1)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=3)\n",
    "    diag = simplex_tree.persistence()\n",
    "    \n",
    "    gudhi.plot_persistence_barcode(diag, legend=True, max_intervals=0)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01a81c-7496-461a-9dbc-808958a2329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistence_diagram(matrice):\n",
    "    '''\n",
    "    max_edge_length = longueur maximale d'une arête pour être inclue dans le complexe de Rips\n",
    "    max_dimension = dimension maximale des simplexes à inclure dans l'arbre\n",
    "    diag est une liste de longueur 42 dont chaque élément est un tuple (classe topologique : 0, 1 ou 2 ; (birth, death))\n",
    "\n",
    "    retourne le diagramme de persistence\n",
    "    '''\n",
    "    rips_complex = gudhi.RipsComplex(distance_matrix=matrice, max_edge_length=1)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=3)\n",
    "    diag = simplex_tree.persistence()\n",
    "\n",
    "    # Retourner le diagramme de persistence\n",
    "    return diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6a31a-a2cb-48b8-890d-38fcbcbee66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_persistence_diag(diag):\n",
    "    '''\n",
    "    Affichage d'un diagramme de persistence\n",
    "    '''\n",
    "    gudhi.plot_persistence_diagram(diag, legend=True, max_intervals=0)\n",
    "    plt.tick_params(axis='both', labelsize=15)\n",
    "    plt.xlabel('Birth', fontsize=15)\n",
    "    plt.ylabel('Death', fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da356e33-dec5-4c72-81ab-0ae77379f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diag(diag):\n",
    "    '''\n",
    "    Modifie l'ordre des éléments du tuple diag.\n",
    "    La classe topologique est mise à la fin.\n",
    "    (birth, death, classe topologique)\n",
    "    '''\n",
    "    reversed_diag = [(tpl[1][0], tpl[1][1], tpl[0]) for tpl in diag]\n",
    "    return reversed_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69ae4e-4df7-4467-a53e-5f2db71e296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def landscape(diag,bins=200):\n",
    "    '''\n",
    "    Calcule et affiche le landscape\n",
    "    '''\n",
    "    reversed_diag = reverse_diag(diag)\n",
    "    diag_array = np.array(reversed_diag)\n",
    "    diag_array = np.expand_dims(diag_array, axis=0)  # Add a new axis\n",
    "\n",
    "    landscape = PersistenceLandscape(n_layers=1, n_bins=bins).fit_transform_plot(diag_array)\n",
    "\n",
    "    return landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3993c3-efde-454d-bc82-49d1b8886500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_landscape(data, num_essai, distance):\n",
    "    '''\n",
    "    Fonction qui récapitule la succession des fonctions précédentes\n",
    "    Prend en entrée un signal entier (pas encore d'essais séparés) et qui\n",
    "    renvoie le landscape de l'essai précisé en argument.\n",
    "    '''\n",
    "    \n",
    "    epochs, label = extract_trials(data)\n",
    "    essai=epochs[num_essai,:,:]\n",
    "    matrice_correlation = matrice_de_correlation(essai)\n",
    "    matrice_distance = correlation_to_distance(matrice_correlation, distance)\n",
    "    #persistence_barcode(matrice_distance)\n",
    "    diag = persistence_diagram(matrice_distance)\n",
    "    landscape_result = landscape(diag,bins=200)\n",
    "\n",
    "    return landscape_result, reverse_diag(diag), matrice_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb651dea-5754-49d6-91fb-b0079c88e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_liste_BO(personne, distance):\n",
    "    '''\n",
    "    Il faut juste choisir la personne sur laquelle on travaille et le type de distance\n",
    "    personne = 'A1', ... 'A60'\n",
    "    distance = '1' ou '2'\n",
    "    '''\n",
    "    \n",
    "    data_path1 = IMDIR + '/' + personne + '/' + personne + '_R1_acquisition.gdf'\n",
    "    data_path2 = IMDIR + '/' + personne + '/' + personne + '_R2_acquisition.gdf'\n",
    "    \n",
    "    raw1 = mne.io.read_raw_gdf(data_path1, preload=True)\n",
    "    data1=raw1.copy()\n",
    "    \n",
    "    raw2 = mne.io.read_raw_gdf(data_path2, preload=True)\n",
    "    data2=raw2.copy()\n",
    "    \n",
    "    epochs1, label1 = extract_trials(raw1)\n",
    "    epochs2, label2 = extract_trials(raw2)\n",
    "    \n",
    "    label1 = label1.tolist()\n",
    "    label2 = label2.tolist()\n",
    "    \n",
    "    labels = label1 + label2\n",
    "    \n",
    "    # Liste qui contient les landscapes des 80 essais en boucles ouvertes\n",
    "    landscapes_BO = []\n",
    "    diag_BO = []\n",
    "    matrice_corr_BO = []\n",
    "    \n",
    "    for num_essai in range (40):\n",
    "        l, diag, matrice_corr= data_to_landscape(data1, num_essai, distance)\n",
    "        landscapes_BO.append(l)\n",
    "        diag_BO.append(diag)\n",
    "        matrice_corr_BO.append(matrice_corr)\n",
    "    for num_essai in range(40):\n",
    "        l, diag, matrice_corr = data_to_landscape(data2, num_essai, distance)\n",
    "        landscapes_BO.append(l)\n",
    "        diag_BO.append(diag)\n",
    "        matrice_corr_BO.append(matrice_corr)\n",
    "\n",
    "\n",
    "    return landscapes_BO, diag_BO, matrice_corr_BO, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701cb8d-a4b6-4183-9f3b-d35fd451d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_liste_BF(personne, distance):\n",
    "    '''\n",
    "    Il faut juste choisir la personne sur laquelle on travaille et le type de distance\n",
    "    personne = 'A1', ... 'A60'\n",
    "    distance = '1' ou '2'\n",
    "    C'est une liste des matrices de corrélation pour les essais effectués en boucle fermée\n",
    "    Ce sont nos données test pour le machine learning\n",
    "    '''\n",
    "    \n",
    "    data_path3 = IMDIR + '/' + personne + '/' + personne + '_R3_onlineT.gdf'\n",
    "    data_path4 = IMDIR + '/' + personne + '/' + personne + '_R4_onlineT.gdf'\n",
    "    data_path5 = IMDIR + '/' + personne + '/' + personne + '_R5_onlineT.gdf'\n",
    "    data_path6 = IMDIR + '/' + personne + '/' + personne + '_R6_onlineT.gdf'\n",
    "\n",
    "    data_paths = [data_path3, data_path4, data_path5, data_path6]\n",
    "    datas = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(4):\n",
    "        raw = mne.io.read_raw_gdf(data_paths[i], preload=True)\n",
    "        data = raw.copy()\n",
    "        epochs, label = extract_trials(raw)\n",
    "        label = label.tolist()\n",
    "        datas.append(data)\n",
    "        labels.extend(label)\n",
    "\n",
    "\n",
    "    # Liste qui contient les landscapes des 40*4=160 essais en boucles fermées\n",
    "    landscapes_BF = []\n",
    "    diag_BF = []\n",
    "    matrice_corr_BF = []\n",
    "\n",
    "    for num_essai in range(len(labels)):   \n",
    "        num_essai2 = num_essai//40\n",
    "        l, diag, matrice_corr= data_to_landscape(datas[num_essai2], num_essai2, distance) #modifier datas en faisant *40\n",
    "        landscapes_BF.append(l)\n",
    "        diag_BF.append(diag)\n",
    "        matrice_corr_BF.append(matrice_corr)\n",
    "\n",
    "    \n",
    "    return landscapes_BF, diag_BF, matrice_corr_BF, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12defb46-2e65-4dc5-af17-f109bf08900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liste_to_matrice(liste_matrices_correlation):\n",
    "    '''\n",
    "    Convertir la liste des matrices de correlation en un tableau numpy\n",
    "    Chaque ligne est une matrice applatie (juste sa supérieure, car symétrie)\n",
    "\n",
    "    ici ce sera pour : liste_matrice = matrice_corr_80_BO\n",
    "    '''\n",
    "    tab_des_matrices = []\n",
    "    for matrice in liste_matrices_correlation : \n",
    "        matrice_list = matrice[np.triu_indices(matrice.shape[0])]\n",
    "        tab_des_matrices.append(matrice_list)\n",
    "\n",
    "    res = np.array(tab_des_matrices)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c85f4b-b87e-41bc-aecb-c7508445de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beti_all(liste_diag):\n",
    "    '''\n",
    "    Enlève le troisième élément du tuple (0, 1 et 2) : ne reste que (birth, death) pour tous les beti numbers\n",
    "    '''\n",
    "\n",
    "    liste_beti = []\n",
    "    for diag in liste_diag:\n",
    "        diag_list = []\n",
    "        for elt in diag :\n",
    "            bd = (elt[0], elt[1])\n",
    "            diag_list.append(bd)\n",
    "        liste_beti.append(diag_list)\n",
    "        \n",
    "    return liste_beti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab751c7-04fb-4cd5-a7bd-3ba87ca13f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_accuracy(kmin, kmax, nb_tirages, matrices_corr, labels, metric = 'minkowski'):\n",
    "    '''\n",
    "    J'effectue plusieurs tirages, c'est-à-dire plusieurs séparation train/test différentes.\n",
    "    Pour chacun de ces tirages, j'observe la précision de l'algorithme knn en fonction de la valeur de k (nombre de voisins)\n",
    "    Dans \"matrice_accuracy\", je stocke sur chaque ligne la précision du tirages pour plusieurs k (nb_lignes = nb_tirages ; nb_colonnes = nb_K)\n",
    "    Je renvoie ensuite la moyenne et l'écart-type des précisions pour chaque nombre de voisins k.\n",
    "    -----------------\n",
    "    La métrique par défaut est celle de 'minkowski', elle est modifiable.\n",
    "    '''\n",
    "\n",
    "    acc_mean = []\n",
    "    acc_std = []\n",
    "    \n",
    "    for k in range (kmin, kmax+1):\n",
    "        accuracy_k = []\n",
    "        for tirages in range(nb_tirages):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(matrices_corr, labels, test_size=0.3)\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, metric = metric)\n",
    "            knn.fit(X_train, y_train)\n",
    "            y_pred = knn.predict(X_test)\n",
    "\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            accuracy_k.append(accuracy)\n",
    "\n",
    "        accuracy_k_array = np.array(accuracy_k)\n",
    "        acc_mean.append(np.mean(accuracy_k_array))\n",
    "        acc_std.append(np.std(accuracy_k_array))\n",
    "    \n",
    "    return acc_mean, acc_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97783eae-c697-4ae8-b4ca-e460a927d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_beti_wassertein(kmin, kmax, nb_tirages, diag, labels):\n",
    "    '''\n",
    "    -------------------------------------------------------\n",
    "    Métrique utilisée : distance de wasserstein\n",
    "    Algorithme de ML : KNN\n",
    "    ---------------------------------------------------------\n",
    "    J'effectue plusieurs tirages, c'est-à-dire plusieurs séparation train/test différentes.\n",
    "    Pour chacun de ces tirages, j'observe la précision de l'algorithme knn en fonction de la valeur de k (nombre de voisins)\n",
    "    Dans \"matrice_accuracy\", je stocke sur chaque ligne la précision du tirages pour plusieurs k (nb_lignes = nb_tirages ; nb_colonnes = nb_K)\n",
    "    Je renvoie ensuite la moyenne et l'écart-type des précisions pour chaque nombre de voisins k.\n",
    "    \n",
    "    '''\n",
    "    liste_beti = beti_all(diag)\n",
    "    \n",
    "    # Cette matrice augmente d'une ligne à chaque tirage\n",
    "    # nb_lignes = nb_tirages ; nb_colonnes = nb_K\n",
    "    matrice_accuracy = np.array([]).reshape(0, kmax-kmin+1)\n",
    "    \n",
    "    for tirages in range(nb_tirages):\n",
    "        beti_train, beti_test, labels_train, labels_test = train_test_split(liste_beti, labels, stratify=labels, test_size=0.3)\n",
    "        \n",
    "        # Calculs des matrices de distance\n",
    "        wasser_dist_train = gudhi.representations.metrics.WassersteinDistance()\n",
    "        wasser_dist_train.fit(X=beti_train)\n",
    "        dist_matrix_wasser_train = wasser_dist_train.transform(beti_train)\n",
    "          \n",
    "        wasser_dist_test = gudhi.representations.metrics.WassersteinDistance()\n",
    "        wasser_dist_test.fit(X=beti_test)\n",
    "        dist_matrix_wasser_test = wasser_dist_test.transform(beti_train)\n",
    "        \n",
    "        accuracy_k= []\n",
    "        for k in range (kmin, kmax+1):  \n",
    "            \n",
    "            # Algo KNN\n",
    "            knn_classifier = KNeighborsClassifier(n_neighbors=k, metric='precomputed')\n",
    "            knn_classifier.fit(dist_matrix_wasser_train, labels_train)\n",
    "            predictions = knn_classifier.predict(dist_matrix_wasser_test.T)\n",
    "                \n",
    "             # Calculer la précision\n",
    "            accuracy = accuracy_score(labels_test, predictions)\n",
    "            accuracy_k.append(accuracy)\n",
    "            \n",
    "        accuracy_k_array = np.array(accuracy_k)\n",
    "\n",
    "        matrice_accuracy = np.vstack([matrice_accuracy, accuracy_k_array])\n",
    "\n",
    "    # calcul de la moyenne et de l'écart-type\n",
    "    acc_mean = np.mean(matrice, axis=0)\n",
    "    acc_std = np.std(matrice, axis=0)\n",
    "\n",
    "    return acc_mean, acc_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eac1d3-e7ae-4c8d-815a-c421112b95bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation_dg(landscapes_list, labels):\n",
    "    '''\n",
    "    En entrée : les landscapes à étudier en liste et les labels (1/2) correspondants\n",
    "    En sortie : renvoie les liste des beti_i pour les essais main gauche et main droite séparés\n",
    "    '''\n",
    "\n",
    "    ## 1. Récuper le numéro des landscapes main droite \"1\"\n",
    "    \n",
    "    num_main_droite = [i for i in range(len(labels)) if labels[i] == 1]\n",
    "    num_main_gauche = [i for i in range(len(labels)) if labels[i] == 2]\n",
    "    \n",
    "    ## 2. Séparer les landscapes en deux listes\n",
    "    \n",
    "    landscapes_main_droite = [landscapes_list[i] for i in num_main_droite]\n",
    "    landscapes_main_gauche = [landscapes_list[i] for i in num_main_gauche]\n",
    "    \n",
    "    ## 3. Séparer les beti1 et les beti2\n",
    "    \n",
    "    beti1_droite= [landscapes_main_droite[land][0][1] for land in range(len(landscapes_main_droite))]\n",
    "    beti1_gauche= [landscapes_main_gauche[land][0][1] for land in range(len(landscapes_main_gauche))]\n",
    "    \n",
    "    #On fait pareil pour les beti2, mais attention : pour les landscapes qui sont de dimension 3\n",
    "    beti2_droite= [landscapes_main_droite[land][0][2] for land in range(len(landscapes_main_droite)) if landscapes_main_droite[land].shape[1]==3]\n",
    "    beti2_gauche= [landscapes_main_gauche[land][0][2] for land in range(len(landscapes_main_gauche)) if landscapes_main_gauche[land].shape[1]==3]\n",
    "\n",
    "\n",
    "    return beti1_droite, beti1_gauche, beti2_droite, beti2_gauche\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e314be-6dcb-428f-9467-c54c07031ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moyenne_beti_numbers(beti1_droite, beti1_gauche, beti2_droite, beti2_gauche):\n",
    "    '''\n",
    "    En entrée : les données des beti numbers 1 et 2 pour la main droite et la main gauche\n",
    "    En sortie : - calcule la moyenne pour chaque catégorie  \n",
    "  \n",
    "    '''\n",
    "\n",
    "    ## 4. Calcul de la moyenne\n",
    "    \n",
    "    #pour beti1\n",
    "    \n",
    "    beti1_droite_array = np.array(beti1_droite)\n",
    "    moy_beti1_droite = np.mean(beti1_droite_array, axis=0)  \n",
    "    \n",
    "    beti1_gauche_array = np.array(beti1_gauche)\n",
    "    moy_beti1_gauche = np.mean(beti1_gauche_array, axis=0)\n",
    "   \n",
    "    \n",
    "    # pour beti 2\n",
    "    \n",
    "    beti2_droite_array = np.array(beti2_droite)\n",
    "    moy_beti2_droite = np.mean(beti2_droite_array, axis=0)\n",
    " \n",
    "    beti2_gauche_array = np.array(beti2_gauche)\n",
    "    moy_beti2_gauche = np.mean(beti2_gauche_array, axis=0)\n",
    "    \n",
    "\n",
    "    return moy_beti1_droite, moy_beti1_gauche, moy_beti2_droite, moy_beti2_gauche\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c509a94-5d95-430f-b47f-9c0d6ac422c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_beti_numbers(beti1_droite, beti1_gauche, beti2_droite, beti2_gauche):\n",
    "    '''\n",
    "    En entrée : les données des beti numbers 1 et 2 pour la main droite et la main gauche\n",
    "    En sortie : - calcule la variance pour chaque catégorie  \n",
    "                \n",
    "    '''\n",
    "\n",
    "    ## 5. Calcul de la variance\n",
    "    \n",
    "    #pour beti1\n",
    "\n",
    "    beti1_droite_array = np.array(beti1_droite)\n",
    "    var_beti1_droite = np.var(beti1_droite_array, axis=0)\n",
    "\n",
    "    beti1_gauche_array = np.array(beti1_gauche)\n",
    "    var_beti1_gauche = np.var(beti1_gauche_array, axis=0)\n",
    "    \n",
    "  \n",
    "    # pour beti 2\n",
    "    \n",
    "    beti2_droite_array = np.array(beti2_droite)\n",
    "    var_beti2_droite = np.var(beti2_droite_array, axis=0)\n",
    "\n",
    "    beti2_gauche_array = np.array(beti2_gauche)\n",
    "    var_beti2_gauche = np.var(beti2_gauche_array, axis=0)\n",
    "\n",
    "    return var_beti1_droite, var_beti1_gauche, var_beti2_droite, var_beti2_gauche\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2642c2-61ff-451c-bfcb-610b750b94e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_beti_numbers(beti1_droite, beti1_gauche, beti2_droite, beti2_gauche):\n",
    "    '''\n",
    "    En entrée : les données des beti numbers 1 et 2 pour la main droite et la main gauche\n",
    "    En sortie : - calcule l'écart-type pour chaque catégorie  \n",
    "                \n",
    "    '''\n",
    "\n",
    "    var_beti1_droite, var_beti1_gauche, var_beti2_droite, var_beti2_gauche = var_beti_numbers(beti1_droite, beti1_gauche, beti2_droite, beti2_gauche)\n",
    "\n",
    "    # on prend la racine carrée élément par élément pour les 4 vecteurs\n",
    "\n",
    "    std_beti1_droite = np.sqrt(var_beti1_droite)\n",
    "    std_beti1_gauche = np.sqrt(var_beti1_gauche)\n",
    "    std_beti2_droite = np.sqrt(var_beti2_droite)\n",
    "    std_beti2_gauche = np.sqrt(var_beti2_gauche)\n",
    "\n",
    "    beti1_droite_array = np.array(beti1_droite)\n",
    "    moy_beti1_droite = np.mean(beti1_droite_array, axis=0)\n",
    "\n",
    "    return std_beti1_droite, std_beti1_gauche, std_beti2_droite, std_beti2_gauche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2800d43e-79b5-4a4b-972e-6f3df9d45df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_with_std(mean, std, title, xlabel, ylabel):\n",
    "    '''\n",
    "    Fonction d'affichage prenant entrée une liste de moyennes et une liste d'écart-type.\n",
    "    Affichage de la moyenne en ligne pleine\n",
    "    Lignes pointillées : moyenne + écart-type et moyenne - écart-type\n",
    "    '''\n",
    "    \n",
    "    x = np.arange(len(mean))  \n",
    "    \n",
    "    plt.plot(x, mean, label='Moyenne')  # Courbe de la moyenne\n",
    "    \n",
    "    # Courbe pour l'écart-type au-dessus\n",
    "    plt.plot(x, mean + std, linestyle='--', label='Moyenne + Écart-type')\n",
    "    \n",
    "    # Courbe pour l'écart-type en dessous\n",
    "    plt.plot(x, mean - std, linestyle='--', label='Moyenne - Écart-type')\n",
    "    \n",
    "    plt.fill_between(x, mean - std, mean + std, color='gray', alpha=0.2)  # Remplissage entre les courbes d'écart-type\n",
    "    \n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c65949-e822-4ee9-8a99-68132b5e5f2f",
   "metadata": {},
   "source": [
    "# Etape 3 : Récupération des données \n",
    "**Remarques :**\n",
    "* Par souci de simplicité, nous avons travaillé uniquement avec la base de donnée A.\n",
    "* Il faut choisir le sujet sur lequel on élabore le modèle.\n",
    "* Attention : ne pas entraîner et tester un modèle avec plusieurs sujets différents ! En effet, les séries temporelles ne sont pas stationnaires c'est-à-dire que la moyenne et la variance ne sont pas constantes. On ne peut pas utiliser un même modèle pour plusieurs personnes différentes. L'objectif est plutôt de trouver une méthode efficace sur un sujet et voir si elle l'est aussi avec les autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4159400-dee3-4390-b2ec-8c8baaf3cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers les données\n",
    "# On se concentre sur les sujets de la base de données A\n",
    "IMDIR = '../BCI_Database/BCI_Database/Signals/DATA_A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cbbeda-1222-49bd-b3dd-de7da3817085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On choisit la personne sur laquelle on travaille\n",
    "personne = 'A19'\n",
    "distance = '1'\n",
    "\n",
    "# BO (2 calibration runs - 80 essais)\n",
    "landscapes_BO, diag_BO, matrice_corr_BO, labels_BO = creation_liste_BO(personne = personne, distance = distance)\n",
    "\n",
    "# BF (4 runs - 160 essais)\n",
    "landscapes_BF, diag_BF, matrice_corr_BF, labels_BF = creation_liste_BF(personne = personne, distance = distance)\n",
    "\n",
    "# Concaténation des BO et des BF\n",
    "diag_all = diag_BO + diag_BF\n",
    "labels_all = labels_BO + labels_BF\n",
    "landscapes_all = landscapes_BO + landscapes_BF\n",
    "matrice_corr_all = matrice_corr_BO + matrice_corr_BF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad779c3-5692-448b-8a94-b0c691203b9e",
   "metadata": {},
   "source": [
    "# Etape 4 : Analyse des données / Machine Learning\n",
    "## Objectif général : discriminer une intention de mouvement de la main droite ou de la main gauche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e73931-8863-43fa-9b81-845459d22bb0",
   "metadata": {},
   "source": [
    "**Remarques introductives :**\n",
    "* Dans ce projet, nous avons utilisé algorithme de K-NN (K-Nearest Neighbors) comme algorithme de Machine Learning. Notons que nous avons dédié notre temps de fin de projet à essayer d'améliorer la précision de KNN pour trouver où étaient nos possibles erreurs d'implémentations et d'hypothèses. De ce fait, nous n'avons pas testé d'autres algorithmes de Machine Learning ; c'est quelque chose qui serait justement intéressant à essayer dans de futures continuations.\n",
    "* Dans la suite, nous avons exposé les trois différentes pistes de discrimination que nous avons explorées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969d1ed-bbdb-44bd-b2f4-784b6ccf669b",
   "metadata": {},
   "source": [
    "## 1. Etude des landscapes\n",
    "\n",
    "**Objectif :** Utiliser la moyenne des nombres de Beti et leur écart-type et réussir à discerner un changement entre les landscapes provenant des essais main droite et ceux des essais main gauche.\n",
    "\n",
    "**Résultats :** Chaque landscape a un axe des abscisses qui lui est propre. Donc lorsqu'on calcule la moyenne des nombres de Beti, les valeurs numériques sont pas correspondantes entre plusieurs landscapes --> ce qu'on obtient n'est pas interprétable. Nous avons décidé de ne pas appliquer d'algorithme de Machine Learning à ces données.\n",
    "\n",
    "**Suite :** Il faudrait créer un axe d'abscisse généralisé à tous les landscapes. Nous avons choisi de ne pas développer cette piste mais de plutôt trouver une représentation plus généralisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d6331-310f-4221-9b6e-a042a5ad0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "beti1_droite, beti1_gauche, beti2_droite, beti2_gauche = separation_dg(landscapes_BO, labels_BO)\n",
    "moy_beti1_droite, moy_beti1_gauche, moy_beti2_droite, moy_beti2_gauche = moyenne_beti_numbers(beti1_droite, beti1_gauche, beti2_droite, beti2_gauche)\n",
    "std_beti1_droite, std_beti1_gauche, std_beti2_droite, std_beti2_gauche = std_beti_numbers(beti1_droite, beti1_gauche, beti2_droite, beti2_gauche)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "#Beti 1\n",
    "plt.subplot(221)\n",
    "plot_mean_with_std(moy_beti1_gauche,std_beti1_gauche, 'Beti1 - Main gauche','x','y')\n",
    "plt.subplot(222)\n",
    "plot_mean_with_std(moy_beti1_droite,std_beti1_droite, 'Beti1 - Main droite','x','y')\n",
    "\n",
    "#Beti 2\n",
    "plt.subplot(223)\n",
    "plot_mean_with_std(moy_beti2_gauche,std_beti2_gauche, 'Beti2 - Main gauche','x','y')\n",
    "plt.subplot(224)\n",
    "plot_mean_with_std(moy_beti2_droite,std_beti2_droite, 'Beti2 - Main droite','x','y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa0ff8-2f5a-4778-91fd-cb635c4ee1bc",
   "metadata": {},
   "source": [
    "## 2. Discriminer les matrices de corrélation\n",
    "\n",
    "**Objectifs :** Trouver une représentation simple et généralisée pour chaque essai (contrairement au landscape). Pourrait-elle réussir à distinguer les deux intentions de mouvement différents ?\n",
    "\n",
    "**Résultats :** La précision du modèle n'est pas satisfaisante.\n",
    "\n",
    "**Suite :** Trouver des types de distance plus appropriés pour nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e392f-74aa-4166-9637-a74f8c302c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmin = 1\n",
    "kmax = 30\n",
    "nb_tirages = 20\n",
    "matrices_corr = liste_to_matrice(matrice_corr_all)\n",
    "labels = labels_all\n",
    "\n",
    "acc_mean1, acc_std1 = find_best_accuracy(kmin, kmax, nb_tirages, matrices_corr, labels, metric = 'cityblock')\n",
    "acc_mean2, acc_std2 = find_best_accuracy(kmin, kmax, nb_tirages, matrices_corr, labels, metric = 'euclidean')\n",
    "acc_mean3, acc_std3 = find_best_accuracy(kmin, kmax, nb_tirages, matrices_corr, labels, metric = 'nan_euclidean') \n",
    "acc_mean4, acc_std4 = find_best_accuracy(kmin, kmax, nb_tirages, matrices_corr, labels, metric = 'l1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d5d960-c0ec-47fc-bc9a-1946ab16b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage\n",
    "\n",
    "title = 'Précision du modèle selon K, moyennée sur ' + str(nb_tirages) + ' tirages \\n metric = '\n",
    "xlabel = 'K : nombre de voisins'\n",
    "ylabel = 'Précision'\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "plt.subplot(221)\n",
    "plot_mean_with_std(np.array(acc_mean1), np.array(acc_std1), title + 'cityblock', xlabel, ylabel)\n",
    "\n",
    "plt.subplot(222)\n",
    "plot_mean_with_std(np.array(acc_mean2), np.array(acc_std2), title + 'euclidean' , xlabel, ylabel)\n",
    "\n",
    "plt.subplot(223)\n",
    "plot_mean_with_std(np.array(acc_mean1), np.array(acc_std1), title + 'nan_euclidean', xlabel, ylabel)\n",
    "\n",
    "plt.subplot(224)\n",
    "plot_mean_with_std(np.array(acc_mean2), np.array(acc_std2), title + 'l1' , xlabel, ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb87fee-51ee-49e6-a88f-77550bf0e4e3",
   "metadata": {},
   "source": [
    "## 3. Des matrices de distance plus appropriées\n",
    "\n",
    "**Objectifs :** Donner à l'algorithme de KNN des matrices de distance pré-calculées pour tenter des meilleures précisions\n",
    "\n",
    "**Résultats :** Les précisions restent assez peu satisfaisantes. On a testé en utilisant que les essais en BO (80 essais) ou que en BF (140 essais). Nous avons essayé avec la concaténation B0+BF par pour une raison qu'on ignore, le programme prend beaucoup plus de temps à tourner (même en réduisant le nombre de tirages, nous n'arrivons pas à obtenir un graphique). Les résultats sont similaires entre les deux types d'essais.\n",
    "\n",
    "**Suite :** Changer d'autres paramètres pour arriver à de meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d66079-6958-4296-8bfd-7d915b00b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmin = 1\n",
    "kmax = 25\n",
    "\n",
    "acc_mean, acc_std = accuracy_beti_wassertein(kmin, kmax, nb_tirages=20, diag = diag_BO, labels = labels_BO)\n",
    "acc_mean2, acc_std2 = accuracy_beti_wassertein(kmin, kmax, nb_tirages=5, diag = diag_BF, labels = labels_BF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078111d5-1242-4b85-8fc2-7df44b761fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracer l'évolution de la précision selon le nombre de voisins\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_mean_with_std(acc_mean, acc_std, 'Précision du modèle selon K, moyennée sur ' + str(20) + ' tirages \\n Distance de Wasserstein - BO uniquement', xlabel, ylabel)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_mean_with_std(acc_mean2, acc_std2, 'Précision du modèle selon K, moyennée sur ' + str(5) + ' tirages \\n Distance de Wasserstein - BF uniquement', xlabel, ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d859beed-0924-4a0f-88f0-0d34a6e799cf",
   "metadata": {},
   "source": [
    "# Conclusion et remerciements\n",
    "Nous concluons ce notebook ici et avec lui le projet TSUNAMI. Ce dernier nous a permis de travailler sur un sujet innovateur avec un objectif concret, ce qui l'a rendu très intéressant. Par ailleurs, il nous a fait découvrir l'analyse topologique, qui est un type de représentation que nous n'avions encore jamais eu l'occasion d'étudier. Ce fut une part non négligeable de notre projet que de nous approprier ces outils de représentation qui nous étaient inconnus. De plus, nous avons eu la chance d'être encadrées par un ingénieur travaillant au CHU et ayant ainsi un point de vue plus pragmatique sur le sujet que nous. En effet, dès le début du projet, Aurélien a bien insisté sur le contexte médical et sur le côté pratique des expériences. Nous avons trouvé cela très utile car nous sommes plus habituées à travailler sur de la théorie ; ainsi, il est important de ne pas oublier que les travaux fournis ont comme objectif de servir dans la vie réelle.\n",
    "\n",
    "Enfin, ce projet nous a aussi introduit au monde de la recherche et aux frustrations qui viennent avec. En effet, nous ne pouvons déclarer avoir terminé ce projet puisqu'il sera toujours possible de le continuer et de l'améliorer. De plus, par manque de temps à la fin du projet, nous n'avons pas réussi à obtenir des résultats assez satisfaisants. Nous sommes néanmoins fières du travail réalisé et des efforts fournis.\n",
    "\n",
    "Nos remerciements vont à Mira RIZKALLAH, à Aurélien LANGHENHOVE et à Maria SARKIS qui nous auront encadrées toujours dans la bonne humeur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c38b3-3dca-4b15-b64c-bbc5b400767a",
   "metadata": {},
   "source": [
    "# Références bibliographiques\n",
    "1. Dreyer P., Roc A., Pillette L., Rimbert S., Lotte F. (2023). *A large EEG database with users' profile information for motor imagery.*\n",
    "\n",
    "2. Gervini Zampieri Centeno E., Moreni G., Vriend C., Douw L., Antônio Nobrega Santos F. (2022). *A hands-on tutorial on network and topological neuroscience*\n",
    "\n",
    "3. Yan Y. et al., *\"Topological EEG-Based Functional Connectivity Analysis for Mental Workload State Recognition\"*, in IEEE Transactions on Instrumentation and Measurement, vol. 72, pp. 1-14, 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
